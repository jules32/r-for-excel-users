--- 
title: "R for the Excel User"
author: "Julie Lowndes & Allison Horst"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is a workshop for RStudio::conf(2020) in San Francisco, California"
---

# Welcome {#welcome}

Excel is a widely used and powerful tool for working with data. As automation, reproducibility, collaboration, and frequent reporting become increasingly expected in data analysis, a good option for Excel users is to extend their workflows with R. Integrating R into data analysis with Excel can bridge the technical gap between collaborators using either software. R enables use of existing tools built for specific tasks and overcomes some limitations that arise when working with large datasets or repeated analyses. This course is for Excel users who want to add or integrate R and RStudio into their existing data analysis toolkit. Participants will get hands-on experience working with data across R, Excel, and Google Sheets, focusing on: data import and export, basic wrangling, visualization, and reporting with RMarkdown. Throughout, we will emphasize conventions and best practices for working reproducibly and collaboratively with data, including naming conventions, documentation, organization, all while “keeping the raw data raw”. Whether you are working in Excel and want to get started in R, already working in R and want tools for working more seamlessly with collaborators who use Excel, or whether you are new to data analysis entirely, this is the course for you! 

If you answer yes to these questions, this course is for you!

- Are you an Excel user who wants to expand your data analysis toolset with R?
- Do you want to bridge analyses between Excel and R, whether working independently or to more easily collaborate with others who use Excel or R? 
- Are you new to data analysis, and looking for a good place to get started?

## Prerequisites

Before the training, please make sure you have done the following: 

1. Download and install **up-to-date versions** of:
    - R: https://cloud.r-project.org
    - RStudio: http://www.rstudio.com/download 
1. Install the Tidyverse 
<!--- https://docs.google.com/document/d/1Imcx8ZropMF5tmLF6As02OJam-r1pNexu5pULczCwMA/edit?ts=5d8ce185 --->    
1. Get comfortable: if you're not in a physical workshop, be set up with two screens if possible. You will be following along in RStudio on your own computer while also watching a virtual training or following this tutorial on your own.



<!---

Add bios, etc

Suggested breakdown for a 2-day workshop: 

|time       |      Day 1|      Day 2|
|:----------|----------:|----------:|
|9-10:30    |  [Motivation](#overview), [R & RStudio, Rmarkdown](#rstudio) |  [Data Wrangling: `tidyr`](#tidyr) |
|break      |  | |
|11-12:30   | [GitHub](#github) | [Programming](#programming) |
|lunch      |  ||
|13:30-15:00 |  [Visualization: `ggplot2`](#ggplot2) | [Collaborating with GitHub](#collaborating) |
|break      |  |  |
|15:30-17:00 |  [Data Wrangling: `dplyr`](#dplyr) | Practice, [Be a champion for open data science](#champion) |

--->

<!--chapter:end:index.Rmd-->

# Overview {#overview}


## Summary (a few sentences)
## Objectives (more detailed, bulletpoints?)

## Resources

R is not only a language, it is an active community of developers, users, and educators (often these traits are in each person). This workshop and book based on many excellent materials created by other members in the R community, who share their work freely to help others learn. Using community materials is how WE learned R, and each chapter of the book will have Resources listed for further reading into the topics we discuss. And, when there is no better way to explain something (ahem Jenny Bryan), we will quote or reference that work directly.

- [What They Forgot to Teach You About R](https://whattheyforgot.org/) — Jenny Bryan & Jim Hester
- [Stat545](https://stat545.com/) — Jenny Bryan & Stat545 TAs
- [Where do Things Live in R?](http://rex-analytics.com/things-live-r-r-excel-users/) REX Analytics
- [](https://blog.shotwell.ca/posts/r_for_excel_users/)

## Overview

Welcome. 

<!---introduce ourselves, how this workshop will work. ---> 

This workshop you will learn hands-on how to begin to interoperate between Excel and R. 

<!---
Trying not to be redundant to other tutorials out there, not the most comprehensive of all things possible with Excel, but setting you up with good practices now and to build on and pass on
--->

A main theme throughout is to produce analyses people can understand and build from — including Future You. 
Not so brittle/sensitive to minor changes.


We will learn and reinforce X main things all at the same time: coding with best practices (R/RStudio), how this relates to operations in Excel, Z. This training will teach these all together to reinforce skills and best practices, and get you comfortable with a workflow that you can use in your own projects. 

### What to expect

<!---

If you answer yes to these questions, this course is for you!

- Are you an Excel user who wants to expand your data analysis toolset with R?
- Do you want to bridge analyses between Excel and R, whether working independently or to more easily collaborate with others who use Excel or R? 
- Are you new to data analysis, and looking for a good place to get started?
--->

This is going to be a fun workshop. 

The plan is to expose you to X that you can have confidence using in your work. You'll be working hands-on and doing the same things on your own computer as we do live on up on the screen. We're going to go through a lot in these two days and it's less important that you remember it all. More importantly, you'll have experience with it and confidence that you can do it. The main thing to take away is that there *are* good ways to work between R and Excel; we will teach you to expect that so you can find what you need and use it! A theme throughout is that tools exist and are being developed by real, and extraordinarily nice, people to meet you where you are and help you do what you need to do. If you expect and appreciate that, you will be more efficient in doing your awesome science.

You are all welcome here, please be respectful of one another. You are encouraged to help each other. 

Everyone in this workshop is coming from a different place with different experiences and expectations. But everyone will learn something new here, because there is so much innovation in the data science world. Instructors and helpers learn something new every time, from each other and from your questions. If you are already familiar with some of this material, focus on how we teach, and how you might teach it to others. Use these workshop materials not only as a reference in the future but also for talking points so you can communicate the importance of these tools to your communities. A big part of this training is not only for you to learn these skills, but for you to also teach others and increase the value and practice of open data science in science as a whole. 



### What you'll learn

TODO: dev 

- Motivation is to bridge and/or get out of excel
- We’re not going to replicate all of your fancy things in R, 
- We use Excel to look at data that we’re reading into R
- Spreadsheets are great; blend data entry with analyses and we’re going to try to help you think about them a bit more distinctively.
- Most important collaborator is future you, and future us



An important theme for this workshop is being deliberate about your analyses and setting things up in a way that will make your analytical life better downstream in the current task, and better when Future You or Future Us revisit it in the future (i.e. avoiding: what happens next? What does this name mean?)

This graphic by Hadley Wickham and Garrett Grolemund in their book [R for Data Science](http://r4ds.had.co.nz/) is simple but incredibly powerful: 

```{r, eval=FALSE, echo=FALSE, out.width="80%"}
knitr::include_graphics("img/r4ds_data-science.png")  
```

You may not have ever thought about analysis in such discrete steps: I certainly hadn't before seeing this. That is partly because in Excel, it can be easy to blend these steps together. We are going to keep them separate, and talk about why. The first step is Import: and implicit in this as a first step is that the data is stored elsewhere and is not manipulated directly, which **keeps the raw data raw**. 

We will be focusing on: <!---TODO more?--->    

- **Import**: `readr`, `readxl` to read raw data stored in CSV or Excel files directly into R
- **Tidy**: `tidyr` to (re)organize rows of data into unique values
- **Transform**: `dplyr` to "wrangle" data based on subsetting by rows or columns, sorting and joining
- **Visualize**: `ggplot2` static plots, using grammar of graphics principles
- **Communicate**
    - `writexl` to export intermediate and final data
    - GitHub File Upload and Issues for online publishing and collaboration


### Emphasizing collaboration

TODO: rewrite/update (from OHI book):

Collaborating efficiently has historically been really hard to do. It's only been the last 20 years or so that we've moved beyond mailing things with the postal service. Being able to email and get feedback on files through track changes was a huge step forward, but it comes with a lot of bookkeeping and reproduciblity issues (did I send that report based on `analysis_final_final.xls` or `analysis_final_usethisone.xls`?). But now, open tools make it much easier to collaborate. 

Working with collaborators in mind is critical for reproducibility. And, your most important collaborator is Future You. This training will introduce best practices using open tools, so that collaboration will become second nature to you!

### By the end of the course...

By the end of this course you’ll produce this report that you can reproduce, which means...
Introduce the problem we will solve. Eg: (just an idea maybe time-series is not a great idea) SMALL PROBLEM. (4 mins)
Show data files, We will discuss our analysis plan (only enough to motivate!) Create a report, that looks great.

<!---TODISCUSS: break this into a new chapter? --->

## RStudio Orientation

Open RStudio for the first time. 

Launch RStudio/R.

```{r, echo=FALSE, out.width="80%"}
knitr::include_graphics("img/RStudio_IDE.png")  
```

Notice the default panes:

  * Console (entire left)
  * Environment/History (tabbed in upper right)
  * Files/Plots/Packages/Help (tabbed in lower right)

FYI: you can change the default location of the panes, among many other things: [Customizing RStudio](https://support.rstudio.com/hc/en-us/articles/200549016-Customizing-RStudio). 


An important first question: **where are we?** 

If you've have opened RStudio for the first time, you'll be in your Home directory. This is noted by the `~/` at the top of the console. You can see too that the Files pane in the lower right shows what is in the Home directory where you are. You can navigate around within that Files pane and explore, but note that you won't change where you are: even as you click through you'll still be Home: `~/`. 


```{r, echo=FALSE, out.width="80%"}
knitr::include_graphics("img/RStudio_IDE_homedir.png")  
```


### RStudio Projects

Create a new Project called 'r-for-excel-users'. File > New Project... > New Directory > New Project. Give your Project a name browse to a place to keep it. And then click to Create Project!

What is a Project? It is a way to organize all of the relevant things you need for an analysis in the same place. This means data, code, figures, notes, etc. 

Why does this matter? Keeping everything you need for your analysis together makes it less brittle and more portable — across people, time, and computers.  

Working directory = no file path/broken path issues. Notice that a folder now appears wherever you saved this project with the same name, and it contains a .Rproj file.

Now that we have our Project, here is an important question: where are we? Now we are in our Project. Everything we do will by default be saved here so we can be nice and organized. 

```{r, echo=FALSE, out.width="80%"}
knitr::include_graphics("img/RStudio_IDE_projdir.png")  
```

<!---TODO build  out more! A folder will show up and you can drag and drop into that folder.--->


### R Console

Watch me work in the Console. 

I can do math: 

```{r math, eval=FALSE}
52*40
365/12
```

*TODO: refine*
 
But like Excel, the power comes not from doing small operations by hand (like 8*22.3), it's by being able to operate on whole suites of numbers and datasets. In Excel, data are stored in the spreadsheet. In R, they are stored in dataframes, and named as variables. 

R stores data in variables, and you give them names. This is a big difference with Excel, where you usually identify data by by its location on the grid, like `$A1:D$20`. (You can do this with Excel by naming ranges of cells, but most people don’t do this.)

Data can be a variety of formats, like numeric and text. 

Let's have a look at some data in R. R has several built-in data sets that we can look at and work with. 

One of these datasets is called `mtcars`. If I write this in the Console, it will print the data in the console.

```{r mtcars, eval=FALSE}
mtcars
```

I can also use RStudio's Viewer to see this in a more familiar-looking format: 

```{r mtcars-view, eval=FALSE}
View(mtcars)
```

This opens the fourth pane of the RStudio IDE; when you work in R you will have all four panes open so this will become a very comforting setup for you.

>The basic R data structure is a vector. You can think of a vector like a column in an Excel spreadsheet with the limitation that all the data in that vector must be of the same type. If it is a character vector, every element must be a character; if it is a logical vector, every element must be TRUE or FALSE; if it’s numeric you can trust that every element is a number. There’s no such constraint in Excel: you might have a column which has a bunch of numbers, but then some explanatory test intermingled with the numbers. This isn’t allowed in R. - https://blog.shotwell.ca/posts/r_for_excel_users/

In the Viewer I can do things like filter or sort. This does not do anything to the actual data, it just changes how you are viewing the data. So even as I explore it, I am not editing or manipulating the data. 

Like Excel, some of the biggest power in R is that there are built-in functions that you can use in your analyses (and, as we'll see, R users can easily create and share functions, and it is this open source developer and contributor community that makes R so awesome).

So let's look into some of these functions. In Excel, there is a "SUM" function to calculate a total. Let's expect that there is the same in R. I will type this into the Console: 

```{r sum-help, eval=FALSE}
?sum
```

A few important things to note: 

1. R is case-sensitive. So "sum" is a completely different thing to "Sum" or "SUM". And this is true for the names of functions, data sets, variable names, and data itself ("blue" vs "Blue"). 

1. RStudio has an autocomplete feature that can help you find the function you're looking for. In many cases it pops up as you type, but you can always type the tab key (above your caps lock key) to prompt the autocomplete. And, bonus: this feature can help you with the case-sensitivity mentioned above: If I start typing "?SU" and press tab, it will show me all options starting with those two letters, regardless of capitalization (although it will start with the capital S options). 

OK but what does typing `?sum` actually *do*?

When I press enter/return, it will open up a help page in the bottom right pane. Help pages vary in detail I find some easier to digest than others. But they all have the same structure, which is helpful to know. The help page tells the name of the package in the top left, and broken down into sections:

 - Description: An extended description of what the function does.
 - Usage: The arguments of the function and their default values.
 - Arguments: An explanation of the data each argument is expecting.
 - Details: Any important details to be aware of.
 - Value: The data the function returns.
 - See Also: Any related functions you might find useful.
 - Examples: Some examples for how to use the function.

When I look at a help page, I start with the description, which might be too in-the-weeds for the level of understanding I need at the offset. For the `sum` page, it is pretty straight-forward and lets me know that yup, this is the function I want.

I next look at the usage and arguments, which give me a more concrete view into what the function does. This syntax looks a bit cryptic but what it means is that you use it by writing sum, and then passing whatever you want to it in terms of data: that is what the "..." means. And the "na.rm=FALSE" means that by default, it will not remove NAs (I read this as: "remove NAs? FALSE!")

Then, I usually scroll down to the bottom to the examples. This is where I can actually see how the function is used, and I can also paste those examples into the Console to see their output. Best way to learn what the function actually does is seeing it in action. Let's try: 

```{r sum, eval=FALSE}
sum(1:5)
```

So this is calculating the sum of the numbers from 1 and 5; that is what that `1:5` syntax means in this case. We can check it with the next example: 

```{r sum2, eval=FALSE}
sum(1, 2, 3, 4, 5)
```

Awesome. Let's try this on our `mtcars` data

```{r sum-mtcars, eval=FALSE}
sum(mtcars)
```

Alright. What is this number? It is the sum of ALL of the data in the mtcars dataset. Maybe in some analysis this would be a useful operation, but I would worry about the way your data is set up and your analyses if this is ever something you'd want to do. More likely, you'd want to take the sum of a specific column. In R, you can do that with the `$` operator. 

Let's say we want to calculate the total number of gears that all these cars have:

```{r sum-mtcars2, eval=FALSE}
sum(mtcars$gear)
```

## Deep thought

How would you do this in Excel? 
The calculations are usually the same shape as the data. In other words if you want to multiply 20 numbers stored in cells A1:An by 2, you will need 20 calculations: =A1 * 2, =A2 * 2, ...., =An * 2.

<!---from https://blog.shotwell.ca/posts/r_for_excel_users/ --->

<!---
TODO: should we get into this?
Let's try another. In Excel there is a function called AVERAGE. Let's see if there is such a thing in R. 
--->

OK so now that we've got a little bit of a feel for R and RStudio, let's do something much more interesting and really start feeling its power. 


## R Scripts

OK so working in the Console is great for quick things, but it gets messy. Keeping  track at what I've done and trying to build upon it would be a nightmare. 

Instead of working in the Console, we can be more organized by writing analyses in a script. This is a really powerful way to work in R. *TODO dev more* Scripts are a written record of the analyses you do, unlike Excel. And they can be re-run easily...

In this script, we're going to make our first figure in R. Let's all do this together. 

<!---TODO add screenshots, build out more ---> 

File > New File > R Script. 

This is a blank slate for us to write our code; but there are some good practices we can start off with. Let's add a useful header to the top of it. For example, at a minimum: 

```{r, eval=FALSE}
# --------------------------------
# A descriptive title
# Your name
# Contact information
# Date
# --------------------------------
```


And then let's save it, naming it something like "my_first_figure.R". Let's get into good habits now with this filename: no spaces! Use underscores `_` or dashes `-` or no space at all. 

Since we're working in or Project, this script is now nicely saved in our Project. You can see our `.R` show up in our Files pane on the bottom right. 

Let's attach a package. Since you've already installed tidyverse, 

```{r, warnings=FALSE, message=FALSE}
# Attach the tidyverse
library(tidyverse)
```


What is the tidyverse? *TODO*
- ggplot2

Let's look at one of the datasets that is built into the ggplot2 package. Type this into your R script: 

```{r, eval=FALSE}
View(diamonds)
```

So this is not immediately executed like when we were typing in the Console. That's because an R script is really just a text file that doesn't do anything on its own; you need to tell R to execute it. You do that in a few ways (let's do each of them):

1. copy-paste this line into the console.
1. click Run (with green arrow at the top-right of your script) to run the line where your cursor is or any highlighted selection
1. click Source (top right of your script) to run the whole script. 

Now let's plot it. Type this or copy-paste and then we'll discuss:

```{r}
ggplot(diamonds, aes(x = carat, y = price)) +
  geom_point()
```


### Deep thought: Error messages are your friends

As [Jenny Bryan says](https://stat545.com/r-basics.html): 

> Implicit contract with the computer / scripting language: Computer will do tedious computation for you. In return, you will be completely precise in your instructions. Typos matter. Case matters. Pay attention to how you type.

Remember that this is a language, not unsimilar to English! There are times you aren't understood -- it's going to happen. There are different ways this can happen. Sometimes you'll get an error. This is like someone saying 'What?' or 'Pardon'? Error messages can also be more useful, like when they say 'I didn't understand what you said, I was expecting you to say blah'. That is a great type of error message. Error messages are your friend. Google them (copy-and-paste!) to figure out what they mean. 

And also know that there are errors that can creep in more subtly, when you are giving information that is understood, but not in the way you meant. Like if I am telling a story about suspenders that my British friend hears but silently interprets in a very different way (true story). This can leave me thinking I've gotten something across that the listener (or R) might silently interpreted very differently. And as I continue telling my story you get more and more confused... Clear communication is critical when you code: write clean, well documented code and check your work as you go to minimize these circumstances!

## Don't save the workspace

## Deep thought: keep the raw data raw. 

Discussing using Excel for variables. 

Horror Stories! Economist etc. 

## Activity 1
## Activity 2
## Efficiency Tips





<!---

## Credit

This material builds from a lot of fantastic materials developed by others in the open data science community. In particular, it pulls from the following resources, which are highly recommended for further learning and as resources later on. Specific lessons will also cite more resources.

- [R for Data Science](http://r4ds.had.co.nz/) by Hadley Wickham and Garrett Grolemund
- [STAT 545](http://stat545.com/) by Jenny Bryan
- [Happy Git with R](http://happygitwithr.com) by Jenny Bryan
- [Software Carpentry](https://software-carpentry.org/lessons/) by the Carpentries

References Brainstorm

- Broman & Woo 2017: https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1375989
--->

<!--chapter:end:overview.Rmd-->

# `readxl` {#readxl}

## Summary

**Check this, may need to be a block quote**: The **readxl** package makes it easy to import tabular data from Excel spreadsheets (.xls or .xlsx files) and includes several options for cleaning data during import. **readxl** has no external dependencies and functions on any operating system, making it an OS- and user-friendly package that simplifies getting your data from Excel into R. 

## Objectives

- Use `readr::read_csv()` to read in a comma separated value (CSV) file
- Use `readxl::read_excel()` to read in an Excel worksheet from a workbook
- Replace a specific string/value in a spreadsheet with with `NA` 
- Skip *n* rows when importing an Excel worksheet
- Use `readxl::read_excel()` to read in parts of a worksheet (by cell range)
- Specify column names when importing Excel data
- Read and combine data from multiple Excel worksheets into a single df using `purrr::map_df()`
- Write data using `readr::write_csv()` or `writexl::write_excel()`
- Workflows with `readxl`: considerations, limitations, reproducibility

## Resources

- https://readxl.tidyverse.org/
- [readxl Workflows article (from tidyverse.org)](https://readxl.tidyverse.org/articles/articles/readxl-workflows.html)


## Lesson

### Lesson prep: get data files into your working directory

In Session 1, we introduced how and why R Projects are great for reproducibility, because our self-contained working directory will be the **first** place R looks for files. 

You downloaded four files for this workshop: 

- fish_counts_curated.csv
- invert_counts_curated.xlsx
- kelp_counts_curated.xlsx
- substrate_cover_curated.xlsx

Copy and paste those files into the 'r-and-excel' folder on your computer. Notice that now these files are in your working directory when you go back to that Project in RStudio (check the 'Files' tab). That means they're going to be in the first place R will look when you ask it to find a file to read in. 

### Create a new .R script, attach the `tidyverse`, `readxl` and `writexl` packages  

In your RforExcelUsers project in RStudio, open a new .R script and add a useful header to the top of it. For example, at a minimum: 

```{r}

# --------------------------------
# A descriptive title
# Summary of what this script is for 
# Your name
# Contact information
# --------------------------------

# Other things you might include: required packages or datasets, relevant links (e.g. to raw data source, GitHub repo, etc.), citations and sources.
```

In this lesson, we'll read in a CSV file with the `readr::read_csv()` function, so we need to have the `readr` package attached. Since it's part of the `tidyverse`, we'll go ahead and attach the `tidyverse` package below our script header using `library(package_name)`. It's a good idea to attach all necessary packages near the top of a script, so we'll also attach the `readxl` packages here. 

```{r, message = FALSE}
# Attach the tidyverse, readxl and writexl packages:
library(tidyverse)
library(readxl)
library(writexl)
```

Now, all of the packages and functions within the `tidyverse` and `readxl`, including `readr::read_csv()` and `readxl::read_excel()`, are available for use. 

### Use `readr::read_csv()` to read in data from a CSV file

There are many types of files containing data that you might want to work with in R. A common one is a comma separated value (CSV) file, which contains values with each column entry separated by a comma delimiter. CSVs can be opened, viewed, and worked with in Excel just like an .xls or .xlsx file - but let's learn how to get data directly from a CSV into R where we can work with it more reproducibly. 

The CSV we'll read in here is called "fish_counts_curated.csv", and contains observations for "the abundance and size of fish species as part of SBCLTER's kelp forest monitoring program to track long-term patterns in species abundance and diversity" from the [Santa Barbara Channel Long Term Ecological Research](http://sbc.lternet.edu/) program. 

**Source:** Reed D. 2018. SBC LTER: Reef: Kelp Forest Community Dynamics: Fish abundance. Environmental Data Initiative. https://doi.org/10.6073/pasta/dbd1d5f0b225d903371ce89b09ee7379. Dataset accessed 9/26/2019.

Read in the "fish_counts_curated.csv" file `read_csv("file_name.csv")`, and store it in R as an object called *fish_counts*:

```{r, include = FALSE}

# Teaching version
fish_counts <- read_csv("curation/fish_counts_curated.csv")

```

```{r, eval = FALSE, results = 'hide'}

fish_counts <- read_csv("fish_counts_curated.csv")

```

Notice that the name of the stored object (here, *fish_counts*) will show up in our Environment tab in RStudio. 

Click on the object in the Environment, and R will automatically run the `View()` function for you to pull up your data in a separate viewing tab. Now we can look at it in the spreadsheet format we're used to. 

Here are a few other functions for quickly exploring imported data: 

- `summary()`: summary of class, dimensions, `NA` values, etc.
- `names()`: variable names (column headers)
- `ls()`: list all objects in environment
- `head()`: Show the first x rows (default is 6 lines)
- `tail()`: Show the last x rows (default is 6 lines)

Now let's make a simple plot of some fish counts with `ggplot2`. 

```{r}
ggplot(fish_counts, aes(x = year, y = tot_count)) +
  geom_col(aes(fill = common_name)) +
  facet_wrap(~site)
```

Now that we have our fish counts data ready to work with in R, let's get the substrate cover and kelp data (both .xlsx files). In the following sections, we'll learn that we can use `readxl::read_excel()` to read in Excel files directly.

### Use `readxl::read_excel()` to read in a single Excel worksheet 

First, take a look at *substrate_cover_curated.xlsx* in Excel, which contains a single worksheet with substrate type and percent cover observations at different sampling locations in the Santa Barbara Channel. 

A few things to notice:

- The file contains a single worksheet
- There are multiple rows containing text information up top
- Where observations were not recorded, there exists '-9999'

Let's go ahead and read in the data. If the file is in our working directory, we can read in a single worksheet .xlsx file using `readxl::read_excel("file_name.xlsx")`. *Note: readxl::read_excel() works for both .xlsx and .xls types*. 

Like this: 
```{r, include = FALSE}

substrate_cover <- read_excel("curation/substrate_cover_curated.xlsx")

```

```{r, eval = FALSE, results = 'hide'}
substrate_cover <- read_excel("substrate_cover_curated.xlsx")
```

**Tada? Not quite.** 

Click on the object name (*substrate_cover*) in the Environment to view the data in a new tab. A few things aren't ideal:

```{r}
substrate_cover
```

- The top row of text has automatically become the (messy) column headers
- There are multiple descriptive rows before we actually get to the data
- There are -9999s that we want R to understand `NA` instead

We can deal with those issues by adding arguments within `read_excel()`. Include argument `skip = n` to skip the first 'n' rows when importing data, and `na = "this"` to replace "this" with `NA` when importing:

```{r, include = FALSE}
substrate_cover <- read_excel("curation/substrate_cover_curated.xlsx", skip = 4, na = "-9999")
```

```{r, eval = FALSE}
substrate_cover <- read_excel("curation/substrate_cover_curated.xlsx, skip = 4, na = "-9999")
```

```{r}
substrate_cover
```


Check out *substrate_cover*, and see that the first row *after* the 4 skipped are the column names, and all -9999s have been updated to `NA`. Hooray!

### Use `readxl::read_excel()` to read in only *part* of an Excel worksheet

We always advocate for leaving the raw data raw, and writing a complete script containing all steps of data wrangling & transformation. But in *some* situations (be careful), you may want to specify a range of cells to read in from an Excel worksheet.

You can specify a range of cells to read in using the `range = ` argument in `read_excel()`. For example, if I want to read in the rectangle from D12:I15 in *substrate_cover_curated.xlsx* - only observations for Carpenteria Beach (Transect 2) in September 2000 - I can use: 
```{r, include = FALSE}
carp_cover_2000 <- readxl::read_excel("curation/substrate_cover_curated.xlsx", range = "D12:I15")
```

```{r, eval = FALSE}
carp_cover_2000 <- readxl::read_excel("substrate_cover_curated.xlsx", range = "D12:I15")
```

But yuck. Look at *carp_cover_2000* and you'll notice that the first row *of that range* is automatically made the column headers. To keep all rows within a range and **add your own column names**, add a `col_names = ` argument:
```{r, include = FALSE}
carp_cover_2000 <- readxl::read_excel("curation/substrate_cover_curated.xlsx", range = "D12:I15", col_names = c("site_name", "transect", "quad", "plot_side", "type", "coverage"))
```

```{r, eval = FALSE}
carp_cover_2000 <- readxl::read_excel("substrate_cover_curated.xlsx", range = "D12:I15", col_names = c("site_name", "transect", "quad", "plot_side", "type", "coverage"))
```

```{r}
carp_cover_2000
```


So far we've read in a single CSV file using `readr::read_csv()`, and an Excel file containing a single worksheet with `readxl::read_excel()`. Now let's read in data from an Excel workbook with multiple worksheets. 

### Use `readxl::read_excel()` to read in selected worksheets from a workbook

Now, we'll read in the kelp fronds data from file *kelp_counts_curated.xlsx*. If you open the Excel workbook, you'll see that it contains multiple worksheets with giant kelp observations in the Santa Barbara Channel during July 2016, 2017, and 2018, with data collected at each *site* in a separate worksheet.

To read in a single Excel worksheet from a workbook we'll again use `readxl::read_excel("file_name.xlsx")`, but we'll need to let R know which worksheet to get. 

Let's read in the kelp data just like we did above, as an object called *kelp_counts*.

```{r, include = FALSE}

# Teaching chunk (won't show up when knitted)
kelp_counts <- readxl::read_excel("curation/kelp_counts_curated.xlsx")

```

```{r, eval = FALSE, results = 'hide'}
kelp_counts <- readxl::read_excel("kelp_counts_curated.xlsx")
```

You might be thinking, "Hooray, I got all of my Excel workbook data!" But remember to always look at your data - you will see that actually only the first worksheet was read in. The default in `readxl::read_excel()` is to read in the **first worksheet** in a multi-sheet Excel workbook. 

To check the worksheet names in an Excel workbook, use `readxl::excel_sheets()`:
```{r, include = FALSE}
readxl::excel_sheets("curation/kelp_counts_curated.xlsx")
```

```{r, eval = FALSE}
readxl::excel_sheets("kelp_counts_curated.xlsx")
```

If we want to read in data from a worksheet other than the first one in an Excel workbook, we can specify the correct worksheet by name or position by adding the `sheet` argument. 

Let's read in data from the worksheet named *golb* (Goleta Beach) in the *kelp_counts_curated.xlsx* workbook: 
```{r, include = FALSE}
kelp_golb <- readxl::read_excel("curation/kelp_counts_curated.xlsx", sheet = "golb")
```

```{r, eval = FALSE, results = 'hide'}
kelp_golb <- readxl::read_excel("kelp_counts_curated.xlsx", sheet = "golb")
```

Note that you can also specify a worksheet by position: since *golb* is the 6^th^ worksheet in the workbook, we could also use the following: 
```{r, include = FALSE}
kelp_golb <- readxl::read_excel("curation/kelp_counts_curated.xlsx", sheet = 6)
```

```{r, eval = FALSE}
kelp_golb <- readxl::read_excel("kelp_counts_curated.xlsx", sheet = 6)
```

```{r}
kelp_golb
```


### Read in and combine data from multiple worksheets into a data frame simultaneously with `purrr::map_df()`

So far, we've read in entire Excel worksheets and pieces of a worksheet. What if we have a workbook (like *kelp_counts_curated.xlsx*) that contains worksheets that contain observations for the same variables, in the same organization? Then we may want to read in data from *all* worksheets, and combine them into a single data frame. 

We'll use `purrr::map_df()` to loop through all the worksheets in a workbook, reading them in & putting them together into a single df in the process. 

The steps we'll go through in the code below are: 

- Set a pathway so that R knows where to look for an Excel workbook
- Get the names of all worksheets in that workbook with `excel_sheets()`
- Set names of a vector with `set_names()`
- Read in all worksheets, and put them together into a single data frame with `purrr::map_df()`

**QUESTION: Have they learned the pipe operator at this point?**

```{r, include = FALSE}

kelp_path <- "curation/kelp_counts_curated.xlsx"

# ADD PIPE OPERATOR BRIEF INTRO (TODO)

kelp_all_sites <- kelp_path %>% # Start with that pathway (file)...
  excel_sheets() %>% # Get all the names of the worksheets
  set_names() %>% # Names the vector items
  map_df(read_excel, path = kelp_path) # Applies 'read_excel' to all worksheet names (loops through them) in the workbook at kelp_path
  
```

**Expect the question:** Why do I need to use read_excel() instead of just giving it the file path (as below)? 

```{r, eval = FALSE}

kelp_path <- "kelp_counts_curated.xlsx"

kelp_all_sites <- kelp_path %>% 
  excel_sheets() %>% 
  set_names() %>% 
  purrr::map_df(read_excel, kelp_path)

```

Check out *kelp_all_sites*, and notice that now the data from all 11 sites is now collected into a single data frame:

```{r}
kelp_all_sites
```

Awesome! Let's make a graph with ggplot2: 
```{r}

ggplot(kelp_all_sites, aes(x = site, y = tot_fronds)) +
  geom_col(aes(fill = year), position = "dodge") +
  scale_fill_manual(values = c("purple","orange","gray30"))
```

```{r, include = FALSE}
# An extra: notice that for site 'bull' only 2 levels show up, so the width of bars is wider. That's obnoxious. We need to complete the levels - we can do that using tidyr::complete(). 

kelp_complete <- kelp_all_sites %>% 
  tidyr::complete(year, site)

# Now, instead of the "bull 2018" observations not existing, it now has 'NA' values. Cool! Then let's try plotting again: 

ggplot(kelp_complete, aes(x = site, y = tot_fronds)) +
  geom_col(aes(fill = year), position = "dodge") +
  scale_fill_manual(values = c("purple","orange","gray30"))

# Now 'bull' has 3 levels again - even though there's only data for two. Good. 

```

### Save data frames as .csv or .xlsx with `readr::write_csv()` or `writexl::write_xlsx()`

There are a number of reasons you might want to save (/export) data in a data frame as a .csv or Excel worksheet, including: 

- To cache raw data within your project
- To store copies of intermediate data frames
- To convert your data back to a format that your coworkers/clients/colleagues will be able to use it more easily

Use `readr::write_csv(object, "file_name.csv")` to write a data frame to a CSV, or `writexl::write_xlsx(object, "file_name.xlsx")` to similarly export as a .xlsx (or .xls) worksheet. 

In the previous step, we combined all of our kelp frond observations into a single data frame. Wouldn't it make sense to store a copy? 

As a CSV: 
```{r, eval = FALSE}
readr::write_csv(kelp_all_sites, "kelp_all_sites.csv")
```

A cool thing about `readr::read_csv()` is that it just quietly *works* without wrecking anything else you do in a sequence, so it's great to add at the end of a piped sequence.

For example, if I want to read in the 'ivee' worksheet from kelp_counts_curated.xlsx, select only columns 'year' and 'tot_fronds', then write that new subset to a .csv file, I can pipe all the way through:
```{r, eval = FALSE}

kelp_ivee <- readxl::read_excel("kelp_counts_curated.xlsx", sheet = "ivee") %>% 
  select(year, tot_fronds) %>% 
  write_csv("kelp_ivee.csv")

```

Now I've created *kelp_ivee.csv*, but the object *kelp_ivee* also exists for me to use in R. 

If needed, I can also export a data frame as an Excel (.xlsx) worksheet:
```{r, eval = FALSE}
writexl::write_xlsx(kelp_all_sites, "kelp_all_sites.xlsx")
```

## Fun facts ideas:

- Did you know that Clippy shows up to help you in the documentation for ?writexl::write_xlsx()?
- The name of the `purrr` package? Why map? 

## Interludes (deep thoughts/openscapes)

- Workflow/reproducibility/readxl workflows article 
- Respecting the tools people are working with already (e.g. don't make your Excel using co-workers hate you)

## Activity: Import some invertebrates! 

There's one dataset we haven't imported or explored yet: invertebrate counts for 5 popular invertebrates (California cone snail, California spiny lobster, orange cup coral, purple urchin and rock scallops) at 11 sites in the Santa Barbara Channel. Take a look at the *invert_counts_curated.xlsx* data by opening it in Excel

- Read in the *invert_counts_curated.xlsx* worksheet as object 'inverts_july', only retaining **site**, **common_name**, and **2016** and setting the existing first row in the worksheet as to column headers upon import
- Explore the imported data frame using View, names, head, tail, etc. 
- Write 'inverts_july' to a CSV file in your working directory called "inverts_july.csv"
- Create a basic graph of invert counts in 2016 (y-axis) by site (x-axis), with each species indicated by a different fill color
    - **Note:** If your column name is a number (not great) you'll probably want to rename it...but in the meantime, to call it as a variable make sure you put single or double quotes around it (e.g. `'2016'` or `"2016"`) so that R recognizes it's a variable and not a value

```{r, include = FALSE}

# Solution:

# Importing only 'site' through '2016' columns: 
inverts_july <- readxl::read_excel("curation/invert_counts_curated.xlsx", range = "B1:D56")

# Do some basic exploring (why might we want to do this in the Console instead?):
#View(inverts_july)
names(inverts_july)
head(inverts_july)
tail(inverts_july)
ls()

# Write back to a csv file: 
# write_csv(inverts_july, "inverts_july.csv")

# Note: this is terrible looking, but fine for the activity
inverts_graph <- ggplot(inverts_july, aes(x = site, y = `2016`)) +
  geom_col(aes(fill = common_name), position = "dodge")
```

```{r, eval = FALSE}
# Importing only 'site' through '2016' columns: 
inverts_july <- readxl::read_excel("curation/invert_counts_curated.xlsx", range = "B1:D56")

# Do some basic exploring (why might we want to do this in the Console instead?):
#View(inverts_july)
names(inverts_july)
head(inverts_july)
tail(inverts_july)
ls()

# Make a gg-graph plot: 
inverts_graph <- ggplot(inverts_july, aes(x = site, y = `2016`)) +
  geom_col(aes(fill = common_name), 
           position = "dodge")
```

```{r}
inverts_graph
```

## Efficiency Tips

- Add an assignment arrow in script/code chunk (<-): Alt + minus (-)
- Undo shortcut: Command + Z 
- Redo shortcut: Command + Shift + Z


<!--chapter:end:readxl.Rmd-->

# RMarkdown {#rmarkdown}

## Summary (a few sentences)
## Objectives (more detailed, bulletpoints?)
## Resources
## Lessons teaching for each objective….. (objectives, examples)

Now, hitting return does not execute this command; remember, it's a text file in the text editor, it's not associated with the R engine. To execute it, we need to get what we typed in the the R chunk (the grey R code) down into the console. How do we do it? There are several ways (let's do each of them):

1. copy-paste this line into the console.
1. select the line (or simply put the cursor there), and click 'Run'. This is available from 
    a. the bar above the file (green arrow)
    b. the menu bar: Code > Run Selected Line(s)
    c. keyboard shortcut: command-return
1. click the green arrow at the right of the code chunk

## Create a Rmd for our Fish analysis

- read in
- View()

fish_counts are what we call Tidy Data. 

## Tidy data

Tidy data has a simple convention: put variables in the columns and observations in the rows.

![](img/tidy_data.png)


## Fun facts (quirky things) - making a note of these wherever possible for interest (little “Did you know?” sections)
## Interludes (deep thoughts/openscapes)
## Interludes (deep thoughts/openscapes)

Comments! Organization (spacing, subsections, vertical structure, indentation, etc.)! Well-named variables! Also, well-named operations so analyses (select(data, columnname)) instead of data[1:6,5] and excel equivalent. (Ex with strings)
Not so brittle/sensitive to minor changes.

## Our Turn Your Turn 1
## Our Turn Your Turn 2
## Efficiency Tips


<!--chapter:end:rmarkdown.Rmd-->

# Dplyr and Pivot Tables {#pivot}

## Summary (a few sentences)

First of two chapters on data wrangling, here focused on pivot tables. 

## Objectives (more detailed, bulletpoints?)

In R, we can use dplyr for pivot tables by using 2 main verbs in combination: `group_by` and `summarize`, and that's where we'll start. Then, we will learn 2 critical verbs that you will use all the time: `select` and `mutate`.

- intro to wrangling with dplyr 
- learn select
- learn group_by + summarize for pivot tables
- emphasize reproducible research


## Resources

## Introduction / our analytical plan

We are going to continue with our analysis with the fish-counts data. So far, our RMarkdown has the following in the "setup" chunk: <!---TODO update after ch 3 complete --->

```{r, eval=FALSE}
## attach libraries
library(tidyverse)

## read in data
fish_counts <- read_csv("fish_counts_curated.csv")
```

```{r, include = FALSE}
# Teaching version
fish_counts <- read_csv("curation/fish_counts_curated.csv")
```

And we have explored the data by looking at some summary statistics and making a simple plot. Now, we are going to learn how to wrangle data in R, using the `dplyr` package which is included in the `tidyverse`. In this session, we'll focus on the functions in `dplyr` that operate like pivot tables.

This is because what we want to do with our fish_counts data is XXXXX. 

## What are pivot tables?

- what they are
- what they allow you to do

Let's talk about how this looks like in R. 

## dplyr for pivot tables

In R, we can use dplyr for pivot tables by using 2 main verbs in combination: `group_by` and `summarize`. Here is what this looks like:

`group_by` and `summarize` are the main ingredients for pivot tables: 

- **`group_by()`**: TODO: words here, screenshot without computed column!

  `r htmltools::img(src='img/rstudio-cheatsheet-summarise.png', width=300)`

- **`summarize()`**: collapse many values down to a single summary 

  `r htmltools::img(src='img/rstudio-cheatsheet-summarise.png', width=300)`
  
  
Let's try this on our fish_counts data. 

```{r}

fish_counts 

```

  
  

- **`select()`**: pick variables by their names

  `r htmltools::img(src='img/rstudio-cheatsheet-select.png', width=300)`
    
- **`mutate()`**: create new variables with functions of existing variables 

  `r htmltools::img(src='img/rstudio-cheatsheet-mutate.png', width=300)`
    
  

All dplyr verbs can be used in conjunction with `group_by()` which changes the scope of each function from operating on the entire dataset to operating on it group-by-group. 

All dplyr verbs work similarly:

1. The first argument is a data frame.
2. The subsequent arguments describe what to do with the data frame. You can refer to columns in the data frame directly without using `$`.
3. The result is a new data frame.

Together these properties make it easy to chain together multiple simple steps to achieve a complex result.


## Deep thought

group_by() %>% summarize() vs
group_by() %>% mutate()







## Fun facts (quirky things) - making a note of these wherever possible for interest (little “Did you know?” sections)
## Interludes (deep thoughts/openscapes)
## Our Turn Your Turn 1
## Our Turn Your Turn 2
## Efficiency Tips


<!--chapter:end:dplyr-pivot-tables.Rmd-->

# Dplyr and vlookups {#vlookup}

## Summary (a few sentences)

In Session 4, we learned how to do some basic wrangling and find summary information with functions in the `dplyr` package, which exists within the `tidyverse`. Those were: 

TODO: Check this list (to see what actually gets covered in Session 4)

- `dplyr::select()`: select which **columns** to retain or exclude
- `dplyr::mutate()`: **add** a new column, while keeping the existing ones
- `dplyr::group_by()`: let R know that **groups** exist within the dataset, by variable(s)
- `dplyr::summarize()`: calculate a value (that you specify) for each group, then report each group's value in a table

In Session 5, we'll expand our data wrangling toolkit using: 

- `dplyr::filter()` to conditionally subset our data by **rows**, and
- `dplyr::join()` functions to merge data frames together

## Objectives

- Continue building R Markdown skills
- Create subsets from data frames by setting conditions for **rows** using `dplyr::filter()` 
- Use `dplyr::full_join()`, `dplyr::inner_join()`, and beyond to merge data frames by matching variables, with different endpoints in mind
- Use `dplyr::anti_join()` to find things that **do not** exist in both data frames
- Understand the similarities between dplyr::filter() + join() and Excel's `VLOOKUP` 

## Resources

- [`dplyr::filter()` documentation from tidyverse.org](https://dplyr.tidyverse.org/reference/filter.html)
- [`dplyr::join()` documentation from tidyverse.org](https://dplyr.tidyverse.org/reference/join.html)
- - [Chapters 5 and 13 in *R for Data Science* by Garrett Grolemund and Hadley Wickham](https://r4ds.had.co.nz/)

## Lessons

**Session 5 set-up:** TODO

- Create a new .Rmd within the r-and-excel directory (project) you created in Session 1
- Add some descriptive text
- Add new code chunks to: 
    - Attach packages
    - Read in the necessary data 
    
In this session we'll use the **fish_counts_curated.csv** and  **invert_counts_curated.xlsx** files. 

```{r, include = FALSE}
library(tidyverse)
library(readxl)
library(here) # They won't have this (files directly in top WD)

# Read in the data
invert_counts <- read_excel(here("curation","invert_counts_curated.xlsx"))

fish_counts <- read_csv(here("curation", "fish_counts_curated.csv"))
```

```{r, eval = FALSE}
# Attach packages:
library(tidyverse)
library(readxl)

# Read in data: 
invert_counts <- read_excel("invert_counts_curated.xlsx")
fish_counts <- read_csv("fish_counts_curated.csv")

```

Remember to always explore the data you've read in using functions like `View()`, `names()`, `summary()`, `head()` and `tail()`. 

Now, let's use `dplyr::filter()` to decide which observations (rows) we'll keep or exclude in new subsets, similar to using Excel's VLOOKUP function.

### `dplyr::filter()` to conditionally subset by rows

Use `dplyr::filter()` to let R know which **rows** you want to keep or exclude, based on what they contain. Some examples in words: 

- "I only want to keep rows where the temperature is greater than 90&deg;F."
- "I want to keep all observations **except** those where the tree type is listed as **unknown**."
- "I want to make a new subset with only data for mountain lions (the species variable) in California (the state variable)."

TODO: {Add filter image here}

When we use `dplyr::filter()`, we need to let R know a couple of things:

- What data frame we're filtering from
- What condition(s) we want observations to **match** and/or **not match** in order to keep them in the new subset

Follow along with the examples below to learn some common ways to use `dplyr::filter()`.

#### Example: filter rows by matching a single character string

Let's say we want to keep all observations from the fish_counts data frame where the common name is "garibaldi." Here, we need to tell R to only *keep rows* from the **fish_counts** data frame when the common name (**common_name** variable) exactly matches **garibaldi**. 

Use `==` to ask R to look for matching strings:

```{r, include = FALSE}
fish_gari <- dplyr::filter(fish_counts, common_name == "garibaldi")
```

```{r, eval = FALSE}
fish_gari <- dplyr::filter(fish_counts, common_name == "garibaldi")
```

Check out the **fish_gari** object to ensure that only *garibaldi* observations remain. 

You could also do this using the pipe operator `%>%` (though for a single function, it doesn't save much effort or typing): 
```{r, eval = FALSE}
fish_gari <- fish_counts %>% 
  dplyr::filter(common_name == "garibaldi")
```

#### Example: filter rows based on numeric conditions

Use expected operators (>, <, >=, <=, =) to set conditions for a numeric variable when filtering. For this example, we only want to retain observations when the **tot_count** column value is >= 50:

```{r, include = FALSE}
fish_over50 <- dplyr::filter(fish_counts, tot_count >= 50)
```

```{r, eval = FALSE}
fish_over50 <- dplyr::filter(fish_counts, tot_count >= 50)
```

Or, using the pipe: 
```{r, eval = FALSE}
fish_over50 <- fish_counts %>% 
  dplyr::filter(tot_count >= 50)
```


#### Example: Make a subset of the fish_counts df that contains *garibaldi*, *blacksmith* OR *black surfperch*

There are several ways to write an "OR" statement, which will keep any observations that match Condition A *or* Condition B *or* Condition C. In this example, we will create a subset from **fish_counts** that only contains rows where the **common_name** is *garibaldi* or *blacksmith* or *black surfperch*.

Use `%in%` to ask R to look for *any matches* within a combined vector of strings: 

```{r, include = FALSE}
fish_3_sp <- fish_counts %>% 
  dplyr::filter(common_name %in% c("garibaldi", "blacksmith", "black surfperch"))
```


#### Example: Make a subset of the fish_counts_curated that does NOT contain observations recorded at Site A

#### Example: Conditionally subset with values

#### Example: Combo w/ string matching & value condition

#### Example: Some combo w/previous functions used

We can also use `dplyr::filter()` in combination with the functions we learned for wrangling yesterday. If we have multiple sequential steps to perform, we can string them together using the *pipe operator* (`%>%`).




####


## Fun facts (quirky things) - making a note of these wherever possible for interest (little “Did you know?” sections)
## Interludes (deep thoughts/openscapes)

- Idea: not overusing the pipe in really long sequences. What are other options? Why is that a concern? What are some ways to always know that what's happening in a sequence is what you EXPECT is happening in a sequence? tidylog, check intermediate data frames, sometimes write intermediate data frames, etc. 



## Our Turn Your Turn 1
## Our Turn Your Turn 2
## Efficiency Tips


<!--chapter:end:dplyr-vlookups.Rmd-->

# Tidying

## Better practices [needs a better name]
How to be a nimble useR
Modern useRs are nimble internet useRs
something clever about cleaning 
I am the worst at naming things

## Summary (a few sentences)

R ecosystem evolves and improves due to contributed work by the community, and this is a good thing. Being a nimble useR means being able to navigate/keep tabs on this ecosystem and find what you need. It also means working reproducibly, so you can re-run and update things more easily. Here we will teach you how to expect things and help yourself. Pay attention to urls.

## Objectives (more detailed, bulletpoints?)

- expect there is a better way, how and where to look (20 mins)
  - CRAN
  - Twitter #rstats
  - rOpenSci
  - RStudio
  - Example: how to Google.
- hands-on with janitor (30+ mins)
  - discovery and quality assurance
  - installing from GitHub
  - big payoff for little effort
- hands-on with another excel-useful example: skimr?   
- reproducibility (20 mins)
  - it's important, scripted


## Resources

- Wilson et al. 2014 "Good enough practices"

## Lessons teaching for each objective….. (objectives, examples)

### Expect there's a better way chat

- give time for them to google?

### Janitor

janitor & other things that will make your life easier with limited effort
Janitor: up till now the column names have been fine. Until now.

#### Our turn your turn
Walk through and example and leave our code up, and have you do it but clean another dataset. Work with a neighbor.

### Example: How to Google
Pay attention to URLs, build github/rmarkdown savviness (ex: raw.githubusercontent.com)

- I read this blog: https://blog.revolutionanalytics.com/2018/08/how-to-use-r-with-excel.html
- I’ve never heard of click on `openxlsx`, what is it
- Takes me here https://www.rdocumentation.org/packages/openxlsx/versions/4.1.0.1, but I want more info. How recently was it worked on? Does it interface with tidyverse? Click on “news”
- Takes me here. https://raw.githubusercontent.com/awalker89/openxlsx/master/NEWS . Not useful. But from this URL, - I see the username so I can edit this url to be
https://github.com/awalker89/openxlsx/
- 1st thing: most recent commit was a year ago. Can poke around more, are there issues open, are they taken care of? Etc. I will probably not pursue using this right now. But good to have learned about it. 

## Fun facts (quirky things) - making a note of these wherever possible for interest (little “Did you know?” sections)
## Interludes (deep thoughts/openscapes)
## Our Turn Your Turn 2
## Efficiency Tips
 
 - browser efficiency tips
   - Rmd/github anchors for urls
   - press command to open a new tab
   

Reproducibility is important (this might be new to some people)
Example: run everything start to finish and then closing it all and trying to do again
In excel
Vs R
If your computer shuts off are you nervous to close it?
Recreate it
“What they didn’t forget to teach you about R” WTDF. uncool 

<!--chapter:end:tidying.Rmd-->

# Formatting and Sharing {#format}

## Summary (a few sentences)
## Objectives (more detailed, bulletpoints?)
## Resources
## Lessons teaching for each objective….. (objectives, examples)

1. Create a GitHub account: https://github.com *Note! Shorter names that kind of identify you are better, and use your work email!*


## Fun facts (quirky things) - making a note of these wherever possible for interest (little “Did you know?” sections)
## Interludes (deep thoughts/openscapes)
## Our Turn Your Turn 1
## Our Turn Your Turn 2
## Efficiency Tips


<!--chapter:end:formatting-and-sharing.Rmd-->

# Synthesis

## Summary (a few sentences)
## Objectives (more detailed, bulletpoints?)
## Resources
## Lessons teaching for each objective….. (objectives, examples)
## Fun facts (quirky things) - making a note of these wherever possible for interest (little “Did you know?” sections)
## Interludes (deep thoughts/openscapes)
## Our Turn Your Turn 1
## Our Turn Your Turn 2
## Efficiency Tips


<!--chapter:end:synthesis.Rmd-->

