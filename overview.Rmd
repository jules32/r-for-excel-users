# Overview {#overview}

```{r setup, include=FALSE}
#knitr::opts_chunk$set(eval = FALSE)
```

## Summary (a few sentences)
## Objectives (more detailed, bulletpoints?)

## Resources

R is not only a language, it is an active community of developers, users, and educators (often these traits are in each person). This workshop and book based on many excellent materials created by other members in the R community, who share their work freely to help others learn. Using community materials is how WE learned R, and each chapter of the book will have Resources listed for further reading into the topics we discuss. And, when there is no better way to explain something (ahem Jenny Bryan), we will quote or reference that work directly.

- [What They Forgot to Teach You About R](https://whattheyforgot.org/) — Jenny Bryan & Jim Hester
- [Stat545](https://stat545.com/) — Jenny Bryan & Stat545 TAs
- [Where do Things Live in R?](http://rex-analytics.com/things-live-r-r-excel-users/) REX Analytics

## Overview

Welcome. 

<!---introduce ourselves ---> 

This workshop you will learn hands-on how to begin to interoperate between Excel and R. 


<!---
Trying not to be redundant to other tutorials out there, not the most comprehensive of all things possible with Excel, but setting you up with good practices now and to build on and pass on
--->

A main theme throughout is to produce analyses people can understand and build from — including Future You. 
Not so brittle/sensitive to minor changes.


We will learn and reinforce X main things all at the same time: coding with best practices (R/RStudio), how this relates to operations in Excel, Z. This training will teach these all together to reinforce skills and best practices, and get you comfortable with a workflow that you can use in your own projects. 

### What to expect

<!---

If you answer yes to these questions, this course is for you!

- Are you an Excel user who wants to expand your data analysis toolset with R?
- Do you want to bridge analyses between Excel and R, whether working independently or to more easily collaborate with others who use Excel or R? 
- Are you new to data analysis, and looking for a good place to get started?
--->

This is going to be a fun workshop. 

The plan is to expose you to X that you can have confidence using in your work. You'll be working hands-on and doing the same things on your own computer as we do live on up on the screen. We're going to go through a lot in these two days and it's less important that you remember it all. More importantly, you'll have experience with it and confidence that you can do it. The main thing to take away is that there *are* good ways to work between R and Excel; we will teach you to expect that so you can find what you need and use it! A theme throughout is that tools exist and are being developed by real, and extraordinarily nice, people to meet you where you are and help you do what you need to do. If you expect and appreciate that, you will be more efficient in doing your awesome science.

You are all welcome here, please be respectful of one another. You are encouraged to help each other. 

Everyone in this workshop is coming from a different place with different experiences and expectations. But everyone will learn something new here, because there is so much innovation in the data science world. Instructors and helpers learn something new every time, from each other and from your questions. If you are already familiar with some of this material, focus on how we teach, and how you might teach it to others. Use these workshop materials not only as a reference in the future but also for talking points so you can communicate the importance of these tools to your communities. A big part of this training is not only for you to learn these skills, but for you to also teach others and increase the value and practice of open data science in science as a whole. 



### What you'll learn

TODO: dev 

- Motivation is to bridge and/or get out of excel
- We’re not going to replicate all of your fancy things in R, 
- We use Excel to look at data that we’re reading into R
- Spreadsheets are great; blend data entry with analyses and we’re going to try to help you think about them a bit more distinctively.
- Most important collaborator is future you, and future us



An important theme for this workshop is being deliberate about your analyses and setting things up in a way that will make your analytical life better downstream in the current task, and better when Future You or Future Us revisit it in the future (i.e. avoiding: what happens next? What does this name mean?)

This graphic by Hadley Wickham and Garrett Grolemund in their book [R for Data Science](http://r4ds.had.co.nz/) is simple but incredibly powerful: 

```{r, eval=FALSE, echo=FALSE, out.width="80%"}
knitr::include_graphics("img/r4ds_data-science.png")  
```

You may not have ever thought about analysis in such discrete steps: I certainly hadn't before seeing this. That is partly because in Excel, it can be easy to blend these steps together. We are going to keep them separate, and talk about why. The first step is Import: and implicit in this as a first step is that the data is stored elsewhere and is not manipulated directly, which **keeps the raw data raw**. 

We will be focusing on: <!---TODO more?--->    

- **Import**: `readr`, `readxl` to read raw data stored in CSV or Excel files directly into R
- **Tidy**: `tidyr` to (re)organize rows of data into unique values
- **Transform**: `dplyr` to "wrangle" data based on subsetting by rows or columns, sorting and joining
- **Visualize**: `ggplot2` static plots, using grammar of graphics principles
- **Communicate**
    - `writexl` to export intermediate and final data
    - GitHub File Upload and Issues for online publishing and collaboration


### Emphasizing collaboration

TODO: rewrite/update (from OHI book):

Collaborating efficiently has historically been really hard to do. It's only been the last 20 years or so that we've moved beyond mailing things with the postal service. Being able to email and get feedback on files through track changes was a huge step forward, but it comes with a lot of bookkeeping and reproduciblity issues (did I send that report based on `analysis_final_final.xls` or `analysis_final_usethisone.xls`?). But now, open tools make it much easier to collaborate. 

Working with collaborators in mind is critical for reproducibility. And, your most important collaborator is Future You. This training will introduce best practices using open tools, so that collaboration will become second nature to you!

### By the end of the course...

By the end of this course you’ll produce this report that you can reproduce, which means...
Introduce the problem we will solve. Eg: (just an idea maybe time-series is not a great idea) SMALL PROBLEM. (4 mins)
Show data files, We will discuss our analysis plan (only enough to motivate!) Create a report, that looks great.

<!---TODISCUSS: break this into a new chapter? --->

## RStudio Orientation

Open RStudio for the first time. 

Launch RStudio/R.

```{r, eval=TRUE, echo=FALSE, out.width="80%"}
knitr::include_graphics("img/RStudio_IDE.png")  
```

Notice the default panes:

  * Console (entire left)
  * Environment/History (tabbed in upper right)
  * Files/Plots/Packages/Help (tabbed in lower right)

FYI: you can change the default location of the panes, among many other things: [Customizing RStudio](https://support.rstudio.com/hc/en-us/articles/200549016-Customizing-RStudio). 


An important first question: **where are we?** 

If you've have opened RStudio for the first time, you'll be in your Home directory. This is noted by the `~/` at the top of the console. You can see too that the Files pane in the lower right shows what is in the Home directory where you are. You can navigate around within that Files pane and explore, but note that you won't change where you are: even as you click through you'll still be Home: `~/`. 


```{r, eval=TRUE, echo=FALSE, out.width="80%"}
knitr::include_graphics("img/RStudio_IDE_homedir.png")  
```


### RStudio Projects

Create a new Project called 'r-for-excel-users'. File > New Project... > New Directory > New Project. Give your Project a name browse to a place to keep it. And then click to Create Project!

What is a Project? It is a way to organize all of the relevant things you need for an analysis in the same place. This means data, code, figures, notes, etc. 

Why does this matter? Keeping everything you need for your analysis together makes it less brittle and more portable — across people, time, and computers.  

Working directory = no file path/broken path issues. Notice that a folder now appears wherever you saved this project with the same name, and it contains a .Rproj file.

Now that we have our Project, here is an important question: where are we? Now we are in our Project. Everything we do will by default be saved here so we can be nice and organized. 

```{r, eval=TRUE, echo=FALSE, out.width="80%"}
knitr::include_graphics("img/RStudio_IDE_projdir.png")  
```

<!---TODO build  out more! A folder will show up and you can drag and drop into that folder.--->


### R Console

Watch me work in the Console. 

I can do math: 

```{r, eval=FALSE}
52*40
365/12
```

*TODO: refine*
 
But like Excel, the power comes not from doing small operations by hand (like 8*22.3), it's by being able to operate on whole suites of numbers and datasets. In Excel, data are stored in the spreadsheet. In R, they are stored in dataframes, and named as variables. 

R stores data in variables, and that data can be a variety of formats, like numeric and text. Let's have a look at some data in R. R has several built-in data sets that we can look at and work with. 

One of these datasets is called `mtcars`. If I write this in the Console, it will print the data in the console.

```{r, eval=FALSE}
mtcars
```

I can also use RStudio's Viewer to see this in a more familiar-looking format: 

```
View(mtcars)
```

This opens the fourth pane of the RStudio IDE. In the Viewer I can do things like filter or sort. This does not do anything to the actual data, it just changes how you are viewing the data. 

There are also functions like in Excel; I can do `?sum` and look at the help pages. 



Average mile per gallon. In excel there is a function called AVERAGE. Let's see 



OK so working in the Console is great, but it gets messy. It's good for quick things. But looking back at what I've done and trying to build upon it would be a nightmare. 

### Deep thought: Error messages are your friends

Implicit contract with the computer / scripting language: Computer will do tedious computation for you. In return, you will be completely precise in your instructions. Typos matter. Case matters. Pay attention to how you type.

Remember that this is a language, not unsimilar to English! There are times you aren't understood -- it's going to happen. There are different ways this can happen. Sometimes you'll get an error. This is like someone saying 'What?' or 'Pardon'? Error messages can also be more useful, like when they say 'I didn't understand what you said, I was expecting you to say blah'. That is a great type of error message. Error messages are your friend. Google them (copy-and-paste!) to figure out what they mean. 

And also know that there are errors that can creep in more subtly, when you are giving information that is understood, but not in the way you meant. Like if I am telling a story about suspenders that my British friend hears but silently interprets in a very different way (true story). This can leave me thinking I've gotten something across that the listener (or R) might silently interpreted very differently. And as I continue telling my story you get more and more confused... Clear communication is critical when you code: write clean, well documented code and check your work as you go to minimize these circumstances!

## R Scripts

Instead of working in the Console that can get messy, we can be more organized. In a script. Let's all do this together. 

<!---TODO add screenshots, build out more ---> 

File > New File > R Script. 

This is a blank slate for us to write our code; but there are some good practices we can start off with. Let's add a useful header to the top of it. For example, at a minimum: 

```{r, eval=FALSE}
# --------------------------------
# A descriptive title
# Summary of what this script is for 
# Your name
# Contact information
# Date
# --------------------------------

# Other things you might include: required packages or datasets, relevant links (e.g. to raw data source, GitHub repo, etc.), citations and sources.
```


Since we're working in or Project, this script is now nicely saved in our Project. 

Let's attach a package. Since you've already installed tidyverse

```{r, eval=FALSE}
# Attach the tidyverse
library(tidyverse)
```


Cool looking plot with diamonds. 

You can run code line by line or sourcing the script

## Don't save the workspace

## Deep thought: keep the raw data raw. 

Discussing using Excel for variables. 

Horror Stories! Economist etc. 


## Interludes (deep thoughts/openscapes)

Comments! Organization (spacing, subsections, vertical structure, indentation, etc.)! Well-named variables! Also, well-named operations so analyses (select(data, columnname)) instead of data[1:6,5] and excel equivalent. (Ex with strings)
Not so brittle/sensitive to minor changes.


## Our Turn Your Turn 1
## Our Turn Your Turn 2
## Efficiency Tips





<!---

## Credit

This material builds from a lot of fantastic materials developed by others in the open data science community. In particular, it pulls from the following resources, which are highly recommended for further learning and as resources later on. Specific lessons will also cite more resources.

- [R for Data Science](http://r4ds.had.co.nz/) by Hadley Wickham and Garrett Grolemund
- [STAT 545](http://stat545.com/) by Jenny Bryan
- [Happy Git with R](http://happygitwithr.com) by Jenny Bryan
- [Software Carpentry](https://software-carpentry.org/lessons/) by the Carpentries

References Brainstorm

- Broman & Woo 2017: https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1375989
--->